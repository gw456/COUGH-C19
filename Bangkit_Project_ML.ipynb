{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bangkit_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gw456/COUGH-C19/blob/main/Bangkit_Project_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wjjCARt4CoY"
      },
      "source": [
        "DOWNLOAD DATASET DI GITHUB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWnFRVPBPZjW"
      },
      "source": [
        "!git clone https://github.com/iiscleap/Coswara-Data.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY2ei-lxUqG4"
      },
      "source": [
        "EKSTRAK DATA WAV DAB JSON DI FOLDER COSWARA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWRARDvq30Kk"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import numpy as np\n",
        "import glob\n",
        "import json\n",
        "import pandas as pd\n",
        "import json\n",
        "import csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PB1aukh4hjPI"
      },
      "source": [
        "'''\n",
        "This script creates a folder \"Extracted_data\" inside which it extracts all the wav files in the directories date-wise\n",
        "'''\n",
        "\n",
        "coswara_data_dir = '/content/Coswara-Data' # Local Path of iiscleap/Coswara-Data Repo\n",
        "extracted_data_dir = os.path.join(coswara_data_dir, 'Extracted_data')  \n",
        "\n",
        "if not os.path.exists(coswara_data_dir):\n",
        "    raise(\"Check the Coswara dataset directory!\")\n",
        "\n",
        "if not os.path.exists(extracted_data_dir):\n",
        "    os.makedirs(extracted_data_dir) # Creates the Extracted_data folder if it doesn't exist\n",
        "\n",
        "dirs_extracted = set(map(os.path.basename,glob.glob('{}/202*'.format(extracted_data_dir))))\n",
        "dirs_all = set(map(os.path.basename,glob.glob('{}/202*'.format(coswara_data_dir))))\n",
        "\n",
        "dirs_to_extract = list(set(dirs_all) - dirs_extracted)\n",
        "\n",
        "print(dirs_to_extract)\n",
        "\n",
        "for d in dirs_to_extract:\n",
        "    p = subprocess.Popen('cat {}/{}/*.tar.gz.* |tar -xvz -C {}/'.format(coswara_data_dir, d, extracted_data_dir), shell=True)\n",
        "    p.wait()\n",
        "\n",
        "\n",
        "print(\"Extraction process complete!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YpWu7fivpca"
      },
      "source": [
        "MEMILIH FILE COUGH-HEAVY.WAV DENGAN KOMORBID BATUK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bq1TMCQgdF4_"
      },
      "source": [
        "Menyiapkan directory yang akan dipindah"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ix8j5ANzryi5"
      },
      "source": [
        "def menyiapkan_directory_pindahan():\n",
        "  coughHeavy_wavfile_dir = os.path.join('/content', 'Cough-Heavy_wavfile')\n",
        "\n",
        "  if not os.path.exists(extracted_data_dir):\n",
        "      raise(\"Check the Coswara dataset directory!\")\n",
        "\n",
        "  if not os.path.exists(coughHeavy_wavfile_dir):\n",
        "      os.makedirs(coughHeavy_wavfile_dir)\n",
        "\n",
        "  dirs_metadata_json = list(map(os.path.abspath, glob.glob('{}/202*/**/metadata.json'.format(extracted_data_dir), \n",
        "                                                           recursive = True)))\n",
        "  dirs_metadata_json = sorted(dirs_metadata_json)\n",
        "\n",
        "  dirs_cough_heavy_wav = list(map(os.path.abspath, glob.glob('{}/202*/**/cough-heavy.wav'.format(extracted_data_dir), \n",
        "                                                           recursive = True)))\n",
        "  dirs_cough_heavy_wav = sorted(dirs_cough_heavy_wav)\n",
        "\n",
        "  print(len(dirs_metadata_json))\n",
        "  print(len(dirs_cough_heavy_wav))\n",
        "\n",
        "  for i in range(600,607):\n",
        "    print(dirs_metadata_json[i])\n",
        "    print(dirs_cough_heavy_wav[i])\n",
        "\n",
        "  return dirs_metadata_json, dirs_cough_heavy_wav, coughHeavy_wavfile_dir\n",
        "\n",
        "dirs_metadata_json, dirs_cough_heavy_wav, coughHeavy_wavfile_dir =menyiapkan_directory_pindahan()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdiPBA3STlVK"
      },
      "source": [
        "#Hapus metadata dalam sub folder, sehingga jumlah data sama\n",
        "!rm /content/Coswara-Data/Extracted_data/20200418/0VpjgRGE5kR8uTEhksBgvFR3xlA2/0VpjgRGE5kR8uTEhksBgvFR3xlA2/metadata.json\n",
        "!rm /content/Coswara-Data/Extracted_data/20200418/19VBCVaFPcWROz6wWui9G9IU9K12/19VBCVaFPcWROz6wWui9G9IU9K12/metadata.json\n",
        "!rm /content/Coswara-Data/Extracted_data/20200418/22oBvWaWwBhbdNfWoshbse1qZ4q2/22oBvWaWwBhbdNfWoshbse1qZ4q2/metadata.json\n",
        "\n",
        "dirs_metadata_json, dirs_cough_heavy_wav, coughHeavy_wavfile_dir =menyiapkan_directory_pindahan()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDFxP0VhdKLm"
      },
      "source": [
        "Memindah directory yang mengandung komorbid batuk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvcRuZRDxaF-"
      },
      "source": [
        "with open('index_dan_label_data.csv', mode = 'w') as file:\n",
        "  data_writer = csv.writer(file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_ALL)\n",
        "  data_writer.writerow(['Index', 'Label'])\n",
        "  for i in range(0, len(dirs_metadata_json)):\n",
        "    f = open(dirs_metadata_json[i])\n",
        "    dictionary = json.load(f)\n",
        "    if \"cough\" in dictionary:\n",
        "      if dictionary[\"covid_status\"] in ['positive_moderate', 'positive_mild', 'positive_asymp']:\n",
        "        data_writer.writerow([i, 1])\n",
        "      else:\n",
        "        data_writer.writerow([i, 0])\n",
        "      f.close()\n",
        "      p = subprocess.Popen('mv {} {}/{}.wav'.format(dirs_cough_heavy_wav[i], \n",
        "                                                  coughHeavy_wavfile_dir, i+1), \n",
        "                                                  shell=True)\n",
        "      p.wait()\n",
        "    else:\n",
        "      f.close()\n",
        "\n",
        "print(\"Data movement process complete!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Oj0f2E-5wnt"
      },
      "source": [
        "PRE-PROCESSING\n",
        "\n",
        "pre-processing yang dilakukan adalah mengubah wav data menjadi time series data, lalu memotongnya menjadi beberapa periode batuk yang dikandung masing-masing data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUJd0XWgFeQ7"
      },
      "source": [
        "MENGKONVERSI KE TIME SERIES DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT0U3SOPH59X"
      },
      "source": [
        "Definisi Fungsi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWulsbI_4Afv"
      },
      "source": [
        "import soundfile as sf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTi-Op35F8F9"
      },
      "source": [
        "def konversi_time_series(file_dir):\n",
        "  data, fs = sf.read(file_dir) \n",
        "  T = 1/fs\n",
        "  t = [k*T for k in range(len(data))]\n",
        "  return data, t\n",
        "\n",
        "def plot_data_time_series(data, t):\n",
        "  list_data = []\n",
        "  for row in data:\n",
        "    list_data.append(row) \n",
        "  plt.plot(t,list_data)\n",
        "  plt.show()\n",
        "\n",
        "def plot_data_csv(filename):\n",
        "  df = pd.read_csv(filename)\n",
        "  fig = go.Figure(go.Scatter(x = df['Waktu'], y = df['Data']))\n",
        "  fig.update_layout(title='Cough Time Series',\n",
        "                   plot_bgcolor='rgb(230, 230,230)',\n",
        "                   showlegend=True)\n",
        "  fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsAJXDErH9qa"
      },
      "source": [
        "Konversi wav file ke time series data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IddN6aoyIDgK"
      },
      "source": [
        "panjang_baru = os.listdir('/content/Cough-Heavy_wavfile')\n",
        "print(len(panjang_baru))\n",
        "\n",
        "coughHeavy_csvfile_dir = os.path.join('/content', 'Cough-Heavy_csvfile')\n",
        "\n",
        "if not os.path.exists(coughHeavy_wavfile_dir):\n",
        "    raise(\"Check the your directory!\")\n",
        "\n",
        "if not os.path.exists(coughHeavy_csvfile_dir):\n",
        "    os.makedirs(coughHeavy_csvfile_dir)\n",
        "    with open('/content/index_dan_label_data.csv') as data_baca:\n",
        "      data_reader = csv.reader(data_baca, delimiter=',')\n",
        "      line_count = 0\n",
        "      for row in data_reader:\n",
        "        if line_count == 0:\n",
        "            line_count += 1\n",
        "        else:\n",
        "            index = str(int(row[0])+1)\n",
        "            data_raw = open('{}/data_raw_wav_{}.csv'.format(coughHeavy_csvfile_dir, index), mode = 'w')\n",
        "            data_writer = csv.writer(data_raw, delimiter=',', quotechar='\"', quoting=csv.QUOTE_ALL)\n",
        "            data_writer.writerow(['Waktu', 'Data'])\n",
        "            file_dikonversi = '{}/{}.wav'.format(coughHeavy_wavfile_dir, index)\n",
        "            bacaan, waktu = konversi_time_series(file_dikonversi)\n",
        "            for i in range(len(bacaan)):\n",
        "              data_writer.writerow([waktu[i], bacaan[i]])\n",
        "            data_raw.close()\n",
        "            line_count += 1\n",
        "        \n",
        "print(\"Time series data accuired!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWRunWPGITWH"
      },
      "source": [
        "Tes Keberhasilan konversi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fv_1ru0PE3pZ"
      },
      "source": [
        "panjang_wav = len(os.listdir('/content/Cough-Heavy_wavfile'))\n",
        "panjang_csv = len(os.listdir('/content/Cough-Heavy_csvfile'))\n",
        "print(panjang_wav == panjang_csv)\n",
        " \n",
        "file_dir = '/content/Cough-Heavy_wavfile/1106.wav'\n",
        "data_1, t_1 = konversi_time_series(file_dir)\n",
        "plot_data_time_series(data_1, t_1)\n",
        " \n",
        "raw_dir = '/content/Cough-Heavy_csvfile/data_raw_wav_1106.csv'\n",
        "plot_data_csv(raw_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A17jCjoIo_m"
      },
      "source": [
        "EKSTRAKSI FITUR STATISTIK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qew33USbZ9Ft"
      },
      "source": [
        "from scipy.linalg import svd\n",
        "import statistics as stat\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJ-jD_UO5r8B"
      },
      "source": [
        "def ekstraksi_fitur_stat(data):\n",
        "  #Reshape np.array into 2 dimension\n",
        "  number = len(data)\n",
        "  factors = []\n",
        " \n",
        "  for whole_number in range(1, number + 1):\n",
        "    if number % whole_number == 0:\n",
        "      factors.append(whole_number)\n",
        " \n",
        "  if len(factors)%2 == 0:\n",
        "    index = (len(factors)//2)-1\n",
        "    M = factors[index]\n",
        "    N = factors[index+1]\n",
        "  else:\n",
        "    index = (len(factors)-1)//2\n",
        "    M = factors[index]\n",
        "    N = M\n",
        "  data_reshape = data.reshape((N,M))\n",
        " \n",
        "  #Calculating Singular Value Decomposition and statistics parameter\n",
        "  U, S, V = svd(data_reshape)\n",
        "  column = np.shape(S)[0]\n",
        "  sum = 0\n",
        "  list_S = []\n",
        "  for row in range (column):\n",
        "    list_S.append(S[row])\n",
        "    sum = sum + S[row]\n",
        " \n",
        "  std = 0\n",
        "  average = sum/column\n",
        "  for i in range(column-1):\n",
        "    std = std + (list_S[i]-average)**2\n",
        "  std = math.sqrt(std/(column-1))\n",
        "  med = stat.median(list_S)\n",
        "  max = np.amax(list_S)\n",
        "  min = np.amin(list_S)\n",
        "  return average, std, med, max, min"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsWhmT4GuZKJ"
      },
      "source": [
        "#Mencari file kosong\n",
        "file_kosong = []\n",
        "path = '/content/Cough-Heavy_csvfile'\n",
        "for files in os.listdir(path):\n",
        "  df = pd.read_csv('{}/{}'.format(path, files))\n",
        "  if df.empty == True:\n",
        "    file_kosong.append(files)\n",
        "print(file_kosong)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtHiwP-NaANL"
      },
      "source": [
        "path_cough_csv = '/content/Cough-Heavy_csvfile'\n",
        "\n",
        "with open(\"/content/Hasil_Fitur_Statistik.csv\", 'w') as file:\n",
        "  write = csv.writer(file, delimiter=',')\n",
        "  #write.writerow(['filename', 'Mean', 'std', 'median', 'max', 'min', 'label'])\n",
        "  for files in os.listdir(path_cough_csv):\n",
        "    print(files)\n",
        "    if files not in file_kosong:\n",
        "      #Mengekstraksi fitur statistik\n",
        "      data_list = list()\n",
        "      line_count = 1\n",
        "      for row in open('{}/{}'.format(path_cough_csv, files), 'r'):\n",
        "        if line_count == 1:\n",
        "          line_count += 1\n",
        "        else:\n",
        "          data_list.append(row[1])\n",
        "      data = np.array(data_list)\n",
        "      average, std, med, max, min = ekstraksi_fitur_stat(data)\n",
        "\n",
        "      #Menyiapkan labelnya\n",
        "      label = open(\"/content/index_dan_label_data.csv\", 'r')\n",
        "      label_data = csv.reader(label, delimiter = ',')\n",
        "      baris_count = 1\n",
        "      for baris in label_data:\n",
        "        if baris_count == 1:\n",
        "          baris_count += 1\n",
        "        else:\n",
        "          baris_count += 1\n",
        "          index = str(int(baris[0])+1)\n",
        "          file_bandingan = 'data_raw_wav_{}.csv'.format(index)\n",
        "          if files == file_bandingan:\n",
        "            labels = baris[1]\n",
        "            print(file_bandingan, labels)\n",
        "            break\n",
        "      label.close()\n",
        "\n",
        "      #Memasukkan data ke csv\n",
        "      write.writerow([files, average, std, med, max, min, labels])\n",
        "\n",
        "print(\"Ekstraksi feature selesai\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypBCjuHk5f8s"
      },
      "source": [
        "MENYIAPKAN DATA LATIH DAN DATA UJI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZm4TXqC7lch"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4ROI_bk-UFD"
      },
      "source": [
        "data_stat = pd.read_csv('/content/Hasil_Fitur_Statistik.csv',\n",
        "                        names = ['Mean', 'std', 'median', 'max', 'min', 'label'])\n",
        "print(data_stat['Mean'].iloc[0])\n",
        "data_stat.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4wCtGaw_En9"
      },
      "source": [
        "data_stat_feature = data_stat.copy()\n",
        "data_stat_label = data_stat_feature.pop('label')\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(data_stat_feature, data_stat_label, test_size=0.2)\n",
        "\n",
        "x_train = tf.convert_to_tensor(x_train)\n",
        "x_test = tf.convert_to_tensor(x_test)\n",
        "y_train = tf.convert_to_tensor(y_train)\n",
        "y_test = tf.convert_to_tensor(y_test)\n",
        "\n",
        "def scaleData(data):       \n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    return scaler.fit_transform(data)\n",
        "\n",
        "x_train = scaleData(x_train)\n",
        "x_test = scaleData(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmgQP86nwtUO"
      },
      "source": [
        "GENERATE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyhtbZcBTOg0"
      },
      "source": [
        "import pathlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89_bVA-XPBNv"
      },
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "         def on_epoch_end(self, epoch, logs={}):# Your Code\n",
        "            if(logs.get('accuracy')>0.64):\n",
        "                print('\\nReached 64% accuracy so cancelling training!')\n",
        "                self.model.stop_training = True\n",
        " \n",
        "callbacks = myCallback()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMc2ZsS8C6NU"
      },
      "source": [
        "model_stat = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation = 'relu', input_shape = [5]),\n",
        "    tf.keras.layers.Dense(128, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(128, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(128, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "model_stat.summary()\n",
        "\n",
        "model_stat.compile(loss = 'binary_crossentropy', optimizer = tf.keras.optimizers.Adam(0.002), metrics = ['accuracy'])\n",
        "history = model_stat.fit(x_train, y_train, epochs = 100, callbacks=[callbacks], validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djJpPVTINc6j"
      },
      "source": [
        "print(history.history.keys())\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Rcz8qlPQw-_"
      },
      "source": [
        "SAVE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7X0ZSTnQq45"
      },
      "source": [
        "export_dir = '/content'\n",
        "tf.saved_model.save(model_stat, export_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxOpEa9WSzGz"
      },
      "source": [
        "# Select mode of optimization\n",
        "mode = \"Speed\" \n",
        "\n",
        "if mode == 'Storage':\n",
        "    optimization = tf.lite.Optimize.OPTIMIZE_FOR_SIZE\n",
        "elif mode == 'Speed':\n",
        "    optimization = tf.lite.Optimize.OPTIMIZE_FOR_LATENCY\n",
        "else:\n",
        "    optimization = tf.lite.Optimize.DEFAULT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSZy3KjpS4JD"
      },
      "source": [
        "# EXERCISE: Use the TFLiteConverter SavedModel API to initialize the converter\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)\n",
        "\n",
        "# Set the optimzations\n",
        "converter.optimizations = [optimization]\n",
        "\n",
        "# Invoke the converter to finally generate the TFLite model\n",
        "tflite_model = converter.convert()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ibsj_N8ETDtq"
      },
      "source": [
        "tflite_model_file = pathlib.Path('./model.tflite')\n",
        "tflite_model_file.write_bytes(tflite_model)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}